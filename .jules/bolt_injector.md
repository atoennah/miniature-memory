# âš¡ Bolt's Journal: The Master Ledger of Conceptual Injections

This journal logs high-value, transferable wisdom injected into the codebase. Each entry represents a successful transformation of a "black box" module into an educational asset.

---

### Entry 1: Demystifying the Transformer Core

**Date:** 2026-01-01T14:07:14Z
**Location:** `training/model.py`
**Injection Summary:**

1.  **Philosophical Anchor:** Injected a header explaining the pedagogical mission of the from-scratch GPT, grounding the code in its educational purpose.
2.  **The Logos of Attention:** Transformed the `Head` class into a masterclass on Scaled Dot-Product Attention. Explicitly documented the `Q, K, V` roles, the mathematical necessity of the scaling factor, and the function of the causal mask. This prevents future developers from treating attention as magic.
3.  **Parallel Realities:** Clarified the "why" of `MultiHeadAttention`. The injection explains that multiple heads are not just for performance but for capturing diverse linguistic relationships in parallel subspaces.
4.  **Architectural Stability:** Added notes to the `Block` class on residual connections and pre-layer normalization, two of the most critical and often misunderstood components for successfully training deep Transformers.

**Wisdom Transfered:** The core components of the GPT model in this repository are no longer undocumented kernels. They are now self-contained, educational modules. This serves as a bulwark against conceptual rot and ensures that future modifications are built upon a solid foundation of understanding.
