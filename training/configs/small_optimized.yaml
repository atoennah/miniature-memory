# NanoGPT-Lite Configuration (Small - Optimized by Bolt)

model:
  n_embd: 256
  n_head: 4
  n_layer: 7
  block_size: 256
  dropout: 0.2

tokenizer:
  type: character-level

training:
  batch_size: 32
  learning_rate: 5e-5  # Reduced LR for stability
  batch_size: 64 # Optimized: Power of 2 for GPU efficiency
  learning_rate: 5e-5 # Optimized: Lowered for stability on small dataset
  betas: [0.9, 0.95]
  epochs: 1
  output_dir: 'training/checkpoints'
  max_steps: 100
  eval_interval: 10

data:
    path: 'dataset/processed/train.txt'

inference:
  precision: FP16
  temperature: 0.8
  top_p: 0.9
  repetition_penalty: 1.1
