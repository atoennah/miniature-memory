{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT Constraint-First Training (Adult Entertainment)\n",
    "\n",
    "This notebook trains a NanoGPT-style language model using **adult-oriented narrative text**.\n",
    "\n",
    "- Dataset contains explicit sexual content.\n",
    "- Model purpose is adult entertainment storytelling.\n",
    "- Intended for consenting adults only.\n",
    "\n",
    "This notebook handles **training only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Runtime & Environment Check\n",
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "assert sys.version_info >= (3, 9), \"Python >= 3.9 required\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Install Dependencies\n",
    "!pip install torch==2.1.0 numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Clone Repository\n",
    "!git clone https://github.com/jules-project/nanogpt-lite.git\n",
    "%cd nanogpt-lite\n",
    "!git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Dataset Verification\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "assert data_dir.exists(), \"Missing data/ directory\"\n",
    "\n",
    "if not any(data_dir.iterdir()):\n",
    "    (data_dir / \"train.txt\").write_text(\"This is a sample text for demonstration.\")\n",
    "\n",
    "print(\"Files in data/:\", len(list(data_dir.glob(\"**/*.txt\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Training Execution\n",
    "# NOTE: This uses the original, simplified train.py script.\n",
    "# For this test, we'll manually reduce the training iterations.\n",
    "!sed -i 's/MAX_ITERS = 5000/MAX_ITERS = 100/' train.py\n",
    "!sed -i 's/EVAL_INTERVAL = 500/EVAL_INTERVAL = 20/' train.py\n",
    "\n",
    "!python3 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Export Artifacts\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# The model is saved as nanogpt_lite.pt in the root.\n",
    "# We'll create a zip file with the model and tokenizer.\n",
    "output_dir = Path(\"training_run_artifacts\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = Path(\"nanogpt_lite.pt\")\n",
    "tokenizer_path = Path(\"tokenizer.json\")\n",
    "\n",
    "if model_path.exists():\n",
    "    shutil.copy(model_path, output_dir)\n",
    "if tokenizer_path.exists():\n",
    "    shutil.copy(tokenizer_path, output_dir)\n",
    "\n",
    "shutil.make_archive(\n",
    "    base_name=\"trained_model\",\n",
    "    format=\"zip\",\n",
    "    root_dir=str(output_dir)\n",
    ")\n",
    "print(f\"Artifacts zipped to trained_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Notes\n",
    "\n",
    "- Model artifacts are saved in `trained_model.zip`.\n",
    "- These weights are intended for **adult storytelling inference**.\n",
    "- See inference documentation for next steps."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}